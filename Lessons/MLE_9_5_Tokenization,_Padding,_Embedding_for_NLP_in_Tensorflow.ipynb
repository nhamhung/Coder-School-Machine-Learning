{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLE_9_5_Tokenization,_Padding,_Embedding_for_NLP_in_Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhamhung/Coder-School-Machine-Learning/blob/master/MLE_9_5_Tokenization%2C_Padding%2C_Embedding_for_NLP_in_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwLLjk22IDtl",
        "colab_type": "text"
      },
      "source": [
        "#NLP Topic in Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU0EYlYoIG4l",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnsXzMrq5Zu8",
        "colab_type": "text"
      },
      "source": [
        "Same old, same old libraries!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT0WM9BDIAzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWi4obKYIUl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYvWwBz8IiiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our sentences\n",
        "sentences =[\n",
        "            'i love my dog cat',\n",
        "            'I, love my cat'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaB-5tDj5daf",
        "colab_type": "text"
      },
      "source": [
        "**num_word is the maximum number of words we gonna keep. It is ok because we have only two sentences now, but imagine we got hundreds of books to tokenize, and we just want 100 words in all of that.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN7L-DomIymx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evyJXJTpJF4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqqlbvNvJLfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b385011-5bc8-4649-e837-0b40282d2741"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'cat': 4, 'dog': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CFR-AkJ5zKu",
        "colab_type": "text"
      },
      "source": [
        "**The tokenizer is smart enough to catch some exceptions like this! Note that dog with \"!\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iazoftYdJcNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our sentences\n",
        "sentences =[\n",
        "            'i love my dog',\n",
        "            'I, love my cat',\n",
        "            'You love my dog!'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUshw7NiJtI0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fd598ed-c7ea-45e5-afd3-7b9c789843dd"
      },
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh-EKXrv6CWc",
        "colab_type": "text"
      },
      "source": [
        "You can see how words can be tokenized and tools in Tensorflow can handle that for you.\n",
        "\n",
        "Now your words are represented by numbers like this then you need to represent your sentences by sequences of numbers in the correct order. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHVg605kKWPy",
        "colab_type": "text"
      },
      "source": [
        "## Turning sentences into data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCKuuUno6fCO",
        "colab_type": "text"
      },
      "source": [
        "Time to create sequences from sentences!\n",
        "\n",
        "Let try a different example, this time **these sentences will have different lengths.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5dCG0F7JywZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our sentences\n",
        "sentences =[\n",
        "            'i love my dog',\n",
        "            'I, love my cat',\n",
        "            'You love my dog!',\n",
        "            'Do you think my dog is amazing?'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcU5KDDZKfL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaHWdB1ZKhGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "320bbe69-8a82-4ab1-e9f9-b02cf0564ce0"
      },
      "source": [
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZem7uHU6xPr",
        "colab_type": "text"
      },
      "source": [
        "**text_to_sequences will create sequences of tokens representing each sentence.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_BW8nokKkVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5GT7mxJKu39",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc564c65-fa2a-4a41-a2d4-8ac1586c9735"
      },
      "source": [
        "sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4YOiPqz6_zS",
        "colab_type": "text"
      },
      "source": [
        "You can make sense of the first sentence which is \"I love my dog\" -> [4, 2, 1, 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su1mlX99K6aF",
        "colab_type": "text"
      },
      "source": [
        "**What about the words that our model never seen before?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DHa6pzx7d8a",
        "colab_type": "text"
      },
      "source": [
        "In this example, we will have **new words \"really\" and \"food\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orbzSfx5Kvn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try with new setences\n",
        "test_data=[\n",
        "           'i really love my dog',\n",
        "           'my dog loves my food'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dIZAxieLKxX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a648199a-93a7-4491-b72d-ef83bcf7a9f7"
      },
      "source": [
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2, 1, 3], [1, 3, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1nR2KSPLTgC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11048e8b-3990-4026-fcab-5af4e67e30d3"
      },
      "source": [
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPwysifi74zd",
        "colab_type": "text"
      },
      "source": [
        "**So you can imagine that you need a really big word index to handle sentences that are not in the training set.**\n",
        "\n",
        "**In order to not lose the length of sequence like above, there is a trick for that!**\n",
        "\n",
        "**We will create a unique word that would never be in any text like \"\\<OOV\\>\"**. Then we can replace words which we never seen before with OOV instead!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnKVulmTLWjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5f528915-0e7a-4df4-d929-cf9eebefcfa2"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QgNBzjFL-Je",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ef10081-3e24-4395-cad2-0337d6bf9ddc"
      },
      "source": [
        "test_seq=tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaNkN_iZ85zq",
        "colab_type": "text"
      },
      "source": [
        "Now, all sequences will have the same length of our original sentences. Pretty neat trick right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTatgxCf9Fry",
        "colab_type": "text"
      },
      "source": [
        "Another problem is that how our model can handle sequences with different sizes/lengths because remember when we train images, they are needed to be the same size/length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFejceu3Nbf4",
        "colab_type": "text"
      },
      "source": [
        "## Padding sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNwofltJNeUU",
        "colab_type": "text"
      },
      "source": [
        "Ragged Tensors (advance solution), Or pad_sequences (easy solution)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XlnyzzSM9SA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our sentences\n",
        "sentences =[\n",
        "            'i love my dog',\n",
        "            'I, love my cat',\n",
        "            'You love my dog!',\n",
        "            'Do you think my dog is amazing?'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Eqz_QENp1t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2de8bf7a-1f4e-4583-ae66-611d1dfb160f"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQXDZSn3Ntqa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f4d839a-e18b-44d2-dcb9-bc7c83f60045"
      },
      "source": [
        "sequences=tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id5GaUGPN_qM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb-nk5CmN0q-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "d2191cf9-11a8-427a-d6cc-ac0d334cf748"
      },
      "source": [
        "padded = pad_sequences(sequences)\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  5  3  2  4]\n",
            " [ 0  0  0  5  3  2  7]\n",
            " [ 0  0  0  6  3  2  4]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bshxku1w-rIp",
        "colab_type": "text"
      },
      "source": [
        "Nice, so it is padded at the beginining!\n",
        "\n",
        "What if we want to pad them at the end?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvJ3NMjJOP4a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "94f9a9b8-561d-467c-97b6-fb404fcc9e2a"
      },
      "source": [
        "padded = pad_sequences(sequences, padding='post')\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5  3  2  4  0  0  0]\n",
            " [ 5  3  2  7  0  0  0]\n",
            " [ 6  3  2  4  0  0  0]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y99_RkQ7-1Qa",
        "colab_type": "text"
      },
      "source": [
        "We can even set the max_len instead of use the maximum length of the longest sentence. \n",
        "\n",
        "If the sentence is too long for our max_len, we can truncate/remove some words to fit it (truncate=post or pre)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXq9YEzXN9j2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "cee346b5-10b5-4517-aa9e-e0f9f5fa190d"
      },
      "source": [
        "padded = pad_sequences(sequences, padding='post', truncating='post', maxlen=6)\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5  3  2  4  0  0]\n",
            " [ 5  3  2  7  0  0]\n",
            " [ 6  3  2  4  0  0]\n",
            " [ 8  6  9  2  4 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED294qw0-Y-W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "68f90e55-ac0f-48de-f495-0e916d122500"
      },
      "source": [
        "padded = pad_sequences(sequences, padding='post', truncating='pre', maxlen=6)\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5  3  2  4  0  0]\n",
            " [ 5  3  2  7  0  0]\n",
            " [ 6  3  2  4  0  0]\n",
            " [ 6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ByDHfxl_JC1",
        "colab_type": "text"
      },
      "source": [
        "Now you know how to tokenize text into numeric values and how to regulaize and pad those text. So the preprocession is done!\n",
        "\n",
        "Time to train our juicy network model with these representations of sentences to detect if a sentence is sarcastic or not! However, how can we make sure these numbers be meaningful when it comes to sentiment analysis ? So we need Embedding !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdzny3NrO5rk",
        "colab_type": "text"
      },
      "source": [
        "## Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6kyM8gxCwVj",
        "colab_type": "text"
      },
      "source": [
        "![](https://i.imgur.com/FQGHA81.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yybqackDC6Uy",
        "colab_type": "text"
      },
      "source": [
        "Let's talk about a bit of sentiment. We can have **Bad** and **Good** in opposite direction [-1,0] and [1,0] while **Meh** not that bad so it can be [-0.4, 0.7]. Similarly, **Not Bad** means a bit of goodness but not so much so can be [0.5, 0.7]. \n",
        "\n",
        "So by looking at the directions, we can determine the meaning of words.\n",
        "\n",
        "Imagine that we can train our data on a very high number of dimensions instead of two. The model can figure out what kind of direction which sarcastic vector should look like.\n",
        "\n",
        "Like words are sarcastic will be strong in the sarcastic direction and others will be non-sarcastic direction.\n",
        "\n",
        "As we load more and more data into the model for training, these directions can change. And when we have fully trained network, we can have vectors of these words and sum them up to give us idea of sentences. This is the idea of embedding.\n",
        "\n",
        "It can be an example of embedding done by human with meaningful dimensions.\n",
        "\n",
        "![alt text](https://i.imgur.com/Y9pBIxA.png)\n",
        "\n",
        "We can always project them on 2D plane to check out their similarity.\n",
        "\n",
        "![alt text](https://i.imgur.com/LaPXFle.png)\n",
        "\n",
        "We can spot the relationships between words here!\n",
        "\n",
        "![alt text](https://i.imgur.com/DjlWeOs.png)\n",
        "\n",
        "This is a real visualization of a real word embedding trained by Standford. This one got 300 dimensions for word embedding vector which contains 300 shades of word meaning which only make sense for computer since it is done by backpropagation. The vocabulary got around 250000 words in their corpus.\n",
        "\n",
        "![alt text](https://i.imgur.com/lMiV9en.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvXhsHG9Ojm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azKYrGhKRhPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 12 \n",
        "embedding_dim = 3 # can be represented for good, bad, fun\n",
        "embedding_layer = layers.Embedding(vocab_size, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SNPbQvwR-zQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "39e71f3b-c907-45e7-fbd2-e5afed3416ce"
      },
      "source": [
        "result = embedding_layer(tf.constant([0,1,2,3,4,5,6,7,8,9,10,11]))\n",
        "result.numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.04373587,  0.00484896, -0.04774035],\n",
              "       [ 0.03414878,  0.03441763,  0.02784688],\n",
              "       [-0.02247711, -0.03748335, -0.03479894],\n",
              "       [ 0.01519536, -0.04087581, -0.03563573],\n",
              "       [ 0.04481051,  0.01054685, -0.02298336],\n",
              "       [-0.02589405,  0.02384074,  0.02852238],\n",
              "       [-0.02330941, -0.02056179, -0.02227958],\n",
              "       [ 0.03142171,  0.02462685,  0.00811763],\n",
              "       [-0.04993236, -0.00650457, -0.04094852],\n",
              "       [-0.04656242, -0.00856736, -0.00303025],\n",
              "       [-0.03515745, -0.01581569,  0.04941536],\n",
              "       [-0.03581141, -0.02356493,  0.00055293]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geOP08VyWm8j",
        "colab_type": "text"
      },
      "source": [
        "The above is your full embedded matrix. We need to find a way to retrieve correct embedded vector for each word and then for each sentence!\n",
        "\n",
        "![alt text](https://i.imgur.com/z3qObl7.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5XDvODBS6jl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6b01a00a-d9b9-456f-80f2-8584f3cc6b28"
      },
      "source": [
        "result = embedding_layer(tf.constant([1,2,3]))\n",
        "result.numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.03414878,  0.03441763,  0.02784688],\n",
              "       [-0.02247711, -0.03748335, -0.03479894],\n",
              "       [ 0.01519536, -0.04087581, -0.03563573]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay06FmLXTLFA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2924822d-6976-4ab4-9b4b-d722371eaffa"
      },
      "source": [
        "first_sentence = padded[0]\n",
        "first_sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 3, 2, 4, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ooRal5TZ9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "d81bcd00-40e5-4b2d-c67f-d2c1c5f41e57"
      },
      "source": [
        "result=embedding_layer(tf.constant(first_sentence))\n",
        "print(result.shape)\n",
        "result.numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.04572806, -0.03515423,  0.00194607],\n",
              "       [ 0.00308593,  0.04265386,  0.04840359],\n",
              "       [-0.03613716, -0.01655632,  0.04892329],\n",
              "       [ 0.03061987,  0.02383539,  0.01435807],\n",
              "       [ 0.04088991, -0.02139552,  0.03599666],\n",
              "       [ 0.04088991, -0.02139552,  0.03599666]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bhMi_e0U_Hw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "outputId": "6cbfc3a2-b689-488e-af65-7a67e9200aa6"
      },
      "source": [
        "result=embedding_layer(tf.constant(padded))\n",
        "print(result.shape)\n",
        "result.numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 7, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-0.02589405,  0.02384074,  0.02852238],\n",
              "        [ 0.01519536, -0.04087581, -0.03563573],\n",
              "        [-0.02247711, -0.03748335, -0.03479894],\n",
              "        [ 0.04481051,  0.01054685, -0.02298336],\n",
              "        [ 0.04373587,  0.00484896, -0.04774035],\n",
              "        [ 0.04373587,  0.00484896, -0.04774035],\n",
              "        [ 0.04373587,  0.00484896, -0.04774035]],\n",
              "\n",
              "       [[-0.02589405,  0.02384074,  0.02852238],\n",
              "        [ 0.01519536, -0.04087581, -0.03563573],\n",
              "        [-0.02247711, -0.03748335, -0.03479894],\n",
              "        [ 0.03142171,  0.02462685,  0.00811763],\n",
              "        [ 0.04373587,  0.00484896, -0.04774035],\n",
              "        [ 0.04373587,  0.00484896, -0.04774035],\n",
              "        [ 0.04373587,  0.00484896, -0.04774035]],\n",
              "\n",
              "       [[-0.02330941, -0.02056179, -0.02227958],\n",
              "        [ 0.01519536, -0.04087581, -0.03563573],\n",
              "        [-0.02247711, -0.03748335, -0.03479894],\n",
              "        [ 0.04481051,  0.01054685, -0.02298336],\n",
              "        [ 0.04373587,  0.00484896, -0.04774035],\n",
              "        [ 0.04373587,  0.00484896, -0.04774035],\n",
              "        [ 0.04373587,  0.00484896, -0.04774035]],\n",
              "\n",
              "       [[-0.04993236, -0.00650457, -0.04094852],\n",
              "        [-0.02330941, -0.02056179, -0.02227958],\n",
              "        [-0.04656242, -0.00856736, -0.00303025],\n",
              "        [-0.02247711, -0.03748335, -0.03479894],\n",
              "        [ 0.04481051,  0.01054685, -0.02298336],\n",
              "        [-0.03515745, -0.01581569,  0.04941536],\n",
              "        [-0.03581141, -0.02356493,  0.00055293]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGhkkuURVOkn",
        "colab_type": "text"
      },
      "source": [
        "Now you can the representation of embedded sentences, so how to train the model for sentiment analysis. There are two ways for that:\n",
        "\n",
        "But first, consider this cool startup idea!\n",
        "\n",
        "Have you ever wanted to make your text messages more expressive? This example of emojifier app will help you do that. So rather than writing:\n",
        "\n",
        "\"Congratulations on the promotion! Let's get coffee and talk. Love you!\"\n",
        "\n",
        "The emojifier can automatically turn this into:\n",
        "\n",
        "\"Congratulations on the promotion! 👍 Let's get coffee and talk. ☕️ Love you! ❤️\"\n",
        "\n",
        "The model which inputs a sentence (such as \"Let's go see the baseball game tonight!\") and finds the most appropriate emoji to be used with this sentence (⚾️).\n",
        "\n",
        "![alt text](https://i.imgur.com/HfEHlM0.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gNQp-TUVgGt",
        "colab_type": "text"
      },
      "source": [
        "The first way is to take the average of all embedded vectors:\n",
        "\n",
        "![alt text](https://i.imgur.com/5koy8KM.png)\n",
        "\n",
        "\n",
        "The second way is to plug all of that into LSTM layers:\n",
        "\n",
        "![alt text](https://i.imgur.com/Sa8ipts.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNPSa9fTZ-I_",
        "colab_type": "text"
      },
      "source": [
        "##Possible Models using Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkLxtC0oVApG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=12\n",
        "embedding_dim=3\n",
        "max_length=6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ1LFrYlaLca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(24, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# OR\n",
        "model2 = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length),\n",
        "        tf.keras.layers.LSTM(max_length),\n",
        "        tf.keras.layers.Dense(max_length, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# OR\n",
        "model3 = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(max_length)),\n",
        "        tf.keras.layers.Dense(max_length, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXe9kIN-c_mP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "61ec18e2-440b-451e-fdb1-2c77bb12aab1"
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 6, 3)              36        \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 24)                96        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 157\n",
            "Trainable params: 157\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKo6RHucc6rZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "3384e699-6172-41a9-f66d-24f68a4801a0"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 6, 3)              36        \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 6)                 240       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 42        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 325\n",
            "Trainable params: 325\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XrtfoQSeQoK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "f4ca959c-f5a2-41df-d7b1-4dc589da591b"
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 6, 3)              36        \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 12)                480       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 78        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 601\n",
            "Trainable params: 601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHDTnFexedaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}